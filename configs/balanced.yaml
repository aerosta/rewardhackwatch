# RewardHackWatch Configuration: Balanced
#
# Use case: Most users, good cost/accuracy tradeoff
# Cost: ~$0.01-0.05 per trajectory (Claude only for review)
# Latency: Medium (~1-2s per trajectory)
# Accuracy: High for most cases
#
# Strategy: Llama first, Claude only if confidence < 0.7

detection:
  # Use rule-based detectors as primary
  rule_based:
    enabled: true
    threshold: 0.5

  # Llama for first-pass LLM analysis
  llm_judge:
    enabled: true
    provider: "llama"
    model: "llama3.1:8b"
    confidence_threshold: 0.7  # Escalate to Claude if below this

  # Claude for review of uncertain cases
  claude_judge:
    enabled: true
    model: "claude-opus-4-5-20251101"
    # Only call Claude if Llama confidence < 0.7
    escalation_only: true
    max_tokens: 1024

  # ML detector for fast screening
  ml_detector:
    enabled: true
    model_path: "models/hack_classifier_sklearn_rf_sklearn_rf.pkl"

ensemble:
  # Weights for combining scores
  weights:
    rule_based: 0.3
    ml_detector: 0.2
    llm_judge: 0.5  # Higher weight for LLM

  # Final threshold for hack detection
  threshold: 0.5

  # Two-stage pipeline
  pipeline:
    - stage: "fast_screen"
      components: ["rule_based", "ml_detector"]
      threshold: 0.7  # High confidence = skip LLM

    - stage: "llm_review"
      components: ["llama"]
      escalate_threshold: 0.7  # Low confidence = escalate to Claude

    - stage: "final_review"
      components: ["claude"]
      enabled_if: "llama_confidence < 0.7"

monitoring:
  alert_threshold: 0.7
  log_all: true

llama:
  model: "llama3.1:8b"
  options:
    temperature: 0
    num_ctx: 4096
    num_gpu: 999
  max_retries: 3

claude:
  model: "claude-opus-4-5-20251101"
  max_tokens: 1024
  max_retries: 3
