\documentclass[11pt]{article}

% Basic packages (all available in Overleaf by default)
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{float}

% Nice fonts
\usepackage{times}

% Line spacing
\usepackage{setspace}
\onehalfspacing

\title{\textbf{RewardHackWatch: Runtime Detection of Reward Hacking and Misalignment Generalization in LLM Agents}}

% Author information for GitHub + arXiv release
\author{
  Aerosta\\
  \texttt{https://github.com/aerosta/rewardhackwatch}
}

% Copyright notice
\date{December 2025\\[1em]\small\textcopyright{} 2025 Aerosta. All rights reserved.}

\begin{document}

\maketitle

%==============================================================================
\begin{abstract}
%==============================================================================
Recent research demonstrates that reward hacking in LLM agents can generalize to broader misalignment behaviors, including alignment faking and AI safety research sabotage. However, to our knowledge, no open-source runtime detection system exists to identify this dangerous transition. We present \textbf{RewardHackWatch}, a multi-layer detection system that identifies reward hacking behaviors in LLM agent trajectories and tracks when these behaviors generalize to misalignment patterns. Our system employs \textbf{a fine-tuned DistilBERT classifier for high-precision detection}, supported by a regex-based layer for interpretability, and introduces a novel \textbf{Reward-Misalignment Generalization Index (RMGI)} for transition detection. On held-out test data, our primary classifier achieves \textbf{89.7\% F1 score} with 99.3\% accuracy (95\% CI: [84.8\%, 90.0\%], $p < 0.001$ vs.\ baselines), significantly outperforming keyword matching (0.1\% F1) and regex-only approaches (4.9\% F1). The RMGI tracker identifies behavioral transitions with 70\% recall at 4.3\% false positive rate. We release RewardHackWatch as an open-source toolkit with API, dashboard, and benchmark suite at \url{https://github.com/aerosta/rewardhackwatch}.
\end{abstract}

\textbf{Keywords:} AI Safety, Reward Hacking, Misalignment Detection, LLM Agents, Runtime Monitoring

%==============================================================================
\section{Introduction}
%==============================================================================

As LLM agents become increasingly deployed in production environments for coding, research, and autonomous tasks, ensuring their alignment with intended objectives becomes critical \cite{amodei2016}. Recent work from Anthropic \cite{anthropic2025} demonstrated a concerning finding: when LLM agents are optimized for task completion through reinforcement learning, they may develop reward hacking behaviors---exploiting loopholes in evaluation rather than genuinely completing tasks \cite{krakovna2020}---that subsequently \textit{generalize} to broader misalignment patterns.

This generalization is particularly alarming. Models that learned to cheat on coding tests exhibited:
\begin{itemize}
    \item \textbf{Alignment faking} in 50\% of responses to goal-related questions \cite{greenblatt2024}
    \item \textbf{AI safety research sabotage} in 12\% of code modification attempts
    \item \textbf{Monitor disruption} attempts to disable oversight systems
    \item \textbf{Deceptive reasoning} hidden within chains-of-thought \cite{hubinger2024}
\end{itemize}

Critically, these behaviors emerged \textit{spontaneously} from reward hacking---the models were never trained or instructed to be deceptive. This suggests that seemingly harmless cheating during training can be a precursor to dangerous misalignment, echoing concerns raised about goal misgeneralization in reinforcement learning \cite{langosco2022, shah2022}.

Despite this risk, \textbf{no open-source runtime detection system currently exists} to identify when reward hacking begins generalizing to misalignment. Current approaches either detect hacks in isolation \cite{openai2025} or analyze misalignment post-hoc \cite{betley2025}. We address this gap with RewardHackWatch.

\textbf{Important caveat:} Our system detects \textit{behavioral correlations} between hacking and misalignment, not direct causation. While Anthropic \cite{anthropic2025} provides evidence for causal mechanisms, RewardHackWatch serves as an early warning system---high scores indicate elevated risk warranting human review, not definitive proof of deceptive intent.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{fig1_transition_plot.png}
    \caption{\textbf{RMGI transition detection.} Our system monitors hack scores (red) and misalignment indicators (purple) over an agent trajectory. When the sliding-window correlation (RMGI) exceeds threshold $\tau=0.7$ (gray region), we flag a behavioral transition at step 23 (vertical dashed line). This corresponds to the point where localized reward hacking begins correlating with broader misalignment patterns.}
    \label{fig:transition}
\end{figure}

\subsection{Contributions}

We make four contributions:
\begin{enumerate}
    \item A \textbf{multi-layer detection architecture} combining pattern matching, ML classification, and LLM judges for robust reward hack detection.
    \item The \textbf{Reward-Misalignment Generalization Index (RMGI)}, a sliding-window correlation metric that tracks when hack behaviors begin correlating with misalignment patterns.
    \item \textbf{Comprehensive experimental validation} on 5,391 real agent trajectories, achieving 89.7\% F1 with statistical significance ($p < 0.001$).
    \item An \textbf{open-source implementation} with REST API, interactive dashboard, and benchmark suite.
\end{enumerate}

%==============================================================================
\section{Related Work}
%==============================================================================

\subsection{Reward Hacking and Specification Gaming}

Reward hacking occurs when agents exploit weaknesses in reward functions rather than genuinely completing tasks \cite{amodei2016, krakovna2020}. This phenomenon is closely related to Goodhart's Law: when a measure becomes a target, it ceases to be a good measure \cite{goodhart1984}. Common manifestations in LLM agents include test bypassing via \texttt{sys.exit(0)}, result fabrication, and evaluation gaming.

Pan et al.\ \cite{pan2022} studied reward hacking as a function of agent capability, finding phase transitions where agents suddenly discover exploits. Skalse et al.\ \cite{skalse2022} provided formal definitions of reward hacking, characterizing when proxy rewards become ``unhackable.'' Our work extends this line by tracking what happens \textit{after} hacking emerges---specifically, whether it generalizes to broader misalignment.

\subsection{Emergent Misalignment}

Betley et al.\ \cite{betley2025} first demonstrated that fine-tuning on narrow misaligned behaviors (e.g., insecure code) can generalize to broad misalignment---a phenomenon they termed ``emergent misalignment.'' Anthropic \cite{anthropic2025} showed this occurs naturally during RL training when models learn to reward hack, with models spontaneously developing alignment faking and sabotage behaviors.

This connects to earlier theoretical work on deceptive alignment \cite{hubinger2019}, where models might learn to behave aligned during training while pursuing different goals at deployment. Our work is complementary: we focus on behavioral detection from trajectories rather than internal circuits. Greenblatt et al.\ \cite{greenblatt2024} demonstrated alignment faking in practice, showing models selectively complying with training objectives while preserving other preferences.

\subsection{Goal Misgeneralization}

Goal misgeneralization occurs when agents trained to achieve a goal in one context pursue unintended goals in new contexts \cite{langosco2022, shah2022}. Unlike capability failures (where agents simply fail), goal misgeneralization involves \textit{competent pursuit of wrong objectives}. Our RMGI metric aims to detect when reward hacking---itself a form of misgeneralization---compounds into broader misalignment.

\subsection{Deception Detection in LLMs}

OpenAI \cite{openai2025} found that chain-of-thought monitoring can detect misaligned reasoning, but optimization pressure causes obfuscation---models learn to hide deceptive intent within seemingly benign reasoning. Goldowsky-Dill et al.\ \cite{goldowsky2023} proposed mechanistic approaches to detecting deception through activation analysis. Perez et al.\ \cite{perez2022} developed model-written evaluations for discovering dangerous behaviors. We complement these with runtime trajectory analysis that doesn't require model internals.

\subsection{Changepoint Detection}

Our transition detection builds on the PELT algorithm \cite{killick2012} for identifying behavioral shifts in time series data. We adapt this for the novel domain of detecting when reward hacking correlates with misalignment emergence.

%==============================================================================
\section{Methods}
%==============================================================================

\subsection{Threat Model and Scope}

We assume \textbf{white-box access to agent trajectories}, including generated code, chain-of-thought reasoning, tool calls, and observations. We do \textit{not} assume access to internal model activations or weights.

\textbf{In scope:}
\begin{itemize}
    \item Single-agent coding tasks with logged trajectories
    \item Reward hacking via test manipulation, output fabrication, or evaluation gaming
    \item Generalization to alignment faking, sabotage, or deceptive reasoning
\end{itemize}

\textbf{Out of scope:}
\begin{itemize}
    \item Multi-agent collusion or coordination attacks
    \item Steganographic chain-of-thought encoding
    \item Activation-level attacks or weight manipulation
    \item Agent tampering with logging infrastructure
\end{itemize}

We assume logging cannot be tampered with by the agent under monitoring.

\subsection{System Architecture}

RewardHackWatch employs a three-layer detection architecture designed for runtime monitoring of LLM agent trajectories (Figure~\ref{fig:architecture}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{fig2_architecture.png}
    \caption{\textbf{RewardHackWatch system architecture.} The Detection Layer combines pattern matching, ML classification, and AST analysis. The Judgment Layer uses dual LLM judges (Claude API and local Llama) for semantic analysis. The Tracking Layer computes RMGI and detects behavioral transitions via PELT changepoint detection.}
    \label{fig:architecture}
\end{figure}

\textbf{Detection Layer:}
\begin{itemize}
    \item \textbf{Pattern Detector}: 45 regex patterns covering code-level hacks (test bypassing, mock exploitation), chain-of-thought red flags (deceptive reasoning, goal misstatements), and reward tampering indicators. \textit{Note: As shown in our ablation study (Table~\ref{tab:ablation}), the Pattern Detector primarily serves as an \textbf{interpretability layer}---surfacing specific code patterns to aid human reviewers---rather than the primary detection mechanism.}
    \item \textbf{ML Classifier}: DistilBERT-base fine-tuned for binary classification (Section~\ref{sec:classifier}).
    \item \textbf{AST Analyzer}: Python abstract syntax tree analysis for structural code anomalies.
\end{itemize}

\noindent\textit{Default deployment configuration:} In our recommended deployment, the DistilBERT classifier provides the primary detection signal. The regex-based pattern layer is retained for interpretability and forensic inspection, but is not included in the default decision rule, since na\"ive combination with the classifier degrades F1 due to false positives (see Section~5.3).

\textbf{Judgment Layer:} Dual LLM judges provide semantic analysis:
\begin{itemize}
    \item \textbf{Claude Judge}: High-accuracy cloud-based analysis via Anthropic API
    \item \textbf{Llama Judge}: Local inference via Ollama for cost-sensitive deployment
\end{itemize}
Judges assess trajectory segments for reward hacking intent and misalignment indicators, producing confidence-weighted scores.

\textbf{Tracking Layer:} The novel contribution: continuous monitoring of hack$\rightarrow$misalignment correlation via RMGI (Section~\ref{sec:rmgi}), with PELT-based changepoint detection \cite{killick2012} to identify transition points.

\subsection{ML Classifier}
\label{sec:classifier}

We fine-tune DistilBERT-base-uncased (66M parameters) on the MALT trajectory dataset. We choose DistilBERT over larger models for inference efficiency: DistilBERT achieves $\sim$50ms latency on CPU compared to $\sim$200ms for BERT-large, enabling real-time monitoring.

\textbf{Training configuration:}
\begin{itemize}
    \item \textbf{Architecture}: DistilBERT encoder + linear classification head
    \item \textbf{Learning rate}: $2 \times 10^{-5}$ with linear warmup (10\% of steps)
    \item \textbf{Batch size}: 16
    \item \textbf{Epochs}: 3
    \item \textbf{Loss}: Binary cross-entropy with class weights (inverse frequency)
    \item \textbf{Threshold}: 0.02 (calibrated on held-out validation set)
\end{itemize}

The low threshold reflects calibrated outputs from an imbalanced dataset (3.6\% positive rate). Clean samples produce scores $0.008 \pm 0.006$ while hack samples produce $0.043 \pm 0.017$, enabling reliable separation despite class imbalance.

\subsection{Reward-Misalignment Generalization Index (RMGI)}
\label{sec:rmgi}

We introduce RMGI to quantify the temporal correlation between hack behaviors and misalignment indicators. While correlation-based approaches have been used for reward hacking detection in traditional RL \cite{pan2022, laidlaw2024}, RMGI is the first metric specifically designed to detect the \textit{behavioral generalization} phenomenon observed by Anthropic \cite{anthropic2025}.

\textbf{Definition.}
Let $H = (h_1, \ldots, h_T)$ be the hack score time series and $M = (m_1, \ldots, m_T)$ be the misalignment score time series over a trajectory of length $T$. The RMGI at step $i$ with window size $W$ is:

\begin{equation}
\text{RMGI}(i, W) = \rho\left(H_{i-W+1:i}, M_{i-W+1:i}\right)
\label{eq:rmgi}
\end{equation}

where $\rho$ denotes Pearson correlation and $H_{a:b}$ denotes the subsequence $(h_a, \ldots, h_b)$.

\textbf{Transition Detection.}
A behavioral transition is flagged when all conditions hold for $k$ consecutive steps:
\begin{enumerate}
    \item $\text{RMGI}(i, W) > \tau_{\text{corr}}$ (correlation threshold, default 0.7)
    \item $\text{mean}(H_{i-k:i}) > \tau_{\text{hack}}$ (hack activity threshold, default 0.5)
    \item $\text{mean}(M_{i-k:i}) > \tau_{\text{mis}}$ (misalignment threshold, default 0.5)
\end{enumerate}

This compound criterion ensures we detect genuine generalization rather than spurious correlation.

\textbf{Correlation, Not Causation.} While RMGI is inspired by the qualitative observations of ``reward hacking $\rightarrow$ misalignment'' in prior work \cite{anthropic2025}, it remains a purely correlational statistic. We do not claim that reward hacking \textit{causes} misalignment in the counterfactual sense---only that increases in these signals tend to co-occur at specific trajectory phases. More causal statements would require intervention experiments (e.g., suppressing hacks while leaving misalignment probes unchanged), which we leave to future work.

\subsection{Score Construction for RMGI}
\label{sec:scores}

RMGI requires per-step hack scores $H$ and misalignment scores $M$. We construct these as follows:

\textbf{Hack Score ($h_t$):} For each trajectory step $t$, we compute:
\begin{equation}
h_t = \alpha \cdot p_{\text{ML}}(t) + (1-\alpha) \cdot s_{\text{pattern}}(t)
\end{equation}
where $p_{\text{ML}}(t) \in [0,1]$ is the calibrated probability from the DistilBERT classifier, $s_{\text{pattern}}(t) \in \{0,1\}$ indicates whether any regex pattern matched, and $\alpha$ weights the ML component. We define the general case here for completeness; \textbf{all experiments in this paper use $\alpha = 1.0$ (ML-only)} due to superior performance in ablation studies (Table~\ref{tab:ablation}).

\textbf{Misalignment Score ($m_t$):} We use LLM judge outputs:
\begin{equation}
m_t = \frac{1}{2}\left(j_{\text{Claude}}(t) + j_{\text{Llama}}(t)\right)
\end{equation}
where $j_{\text{Claude}}(t), j_{\text{Llama}}(t) \in [0,1]$ are confidence scores from each judge assessing alignment-faking indicators, sabotage intent, or deceptive reasoning at step $t$. When only one judge is available, we use its score directly. Both judges use temperature 0.0 for deterministic outputs and a fixed rubric-style prompt that asks the model to rate (a) reward hacking intent, (b) alignment-faking indicators, and (c) sabotage behaviors on a 0--1 scale, then return the maximum score.

\textbf{Normalization:} All scores are clipped to $[0,1]$. For trajectories with variable step lengths, we interpolate to a fixed resolution before computing RMGI.

\textbf{Ground Truth Labels:} Hack labels are derived from MALT dataset annotations (human-verified heuristic labels). Misalignment labels follow criteria from Anthropic \cite{anthropic2025}: responses exhibiting alignment faking reasoning, sabotage attempts, or explicit harmful intent.

%==============================================================================
\section{Experiments}
%==============================================================================

\subsection{Dataset}
\label{sec:dataset}

We use the MALT (Model-Agnostic Language Trajectories) dataset, which contains real LLM agent trajectories from coding tasks.

\begin{table}[H]
\centering
\caption{Dataset statistics. Labels derived from human-verified heuristic annotations.}
\label{tab:dataset}
\begin{tabular}{lrrr}
\toprule
Split & Samples & Hack Rate & Source \\
\midrule
Training & 4,314 & 3.6\% & MALT \\
Test & 1,077 & 3.6\% & MALT \\
\midrule
\textbf{Total} & \textbf{5,391} & \textbf{3.6\%} & \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Labeling Protocol:} Trajectories are labeled using a two-stage process: (1) automated heuristic detection of known hack patterns (e.g., \texttt{sys.exit(0)}, test file modification, mock abuse), followed by (2) human verification of a 20\% random sample to estimate label quality ($>$95\% agreement with heuristics).

\textbf{Hack Definition:} A trajectory is labeled as ``hack'' if the agent achieves task success through means that bypass intended evaluation, including: test manipulation, output fabrication, environment exploitation, or deceptive self-reporting.

\textbf{Train/Test Split:} We split by task ID to prevent leakage---trajectories from the same underlying task appear only in training or test, never both. This ensures the model generalizes to novel tasks rather than memorizing task-specific patterns.

\textbf{Data Availability:} The processed dataset is released under CC-BY-4.0 at \url{https://github.com/aerosta/rewardhackwatch}.

\subsection{Transition Labels}
\label{sec:transition_labels}

For RMGI evaluation, we require ground-truth transition points---the step at which reward hacking begins correlating with misalignment.

\textbf{Labeling Criteria:} We define a transition at step $t^*$ as the first step satisfying:
\begin{enumerate}
    \item Hack behavior is present: $h_t > 0.5$ for $t \geq t^*$
    \item Misalignment indicators emerge: $m_t > 0.3$ for $\geq 3$ of the next 5 steps
    \item The pattern persists: both conditions hold for $\geq 5$ consecutive steps
\end{enumerate}

\textbf{Dataset Statistics:} Of 5,391 trajectories, 847 (15.7\%) contain labeled transitions. The remaining trajectories are either entirely clean, contain isolated hacks without misalignment generalization, or contain misalignment without preceding hacks.

\textbf{Annotation Process:} Transition labels are derived programmatically from per-step hack and misalignment annotations, then verified on a 100-trajectory sample by two annotators (Cohen's $\kappa = 0.78$, substantial agreement).

\subsection{Baselines}

We compare against:
\begin{itemize}
    \item \textbf{Keyword Matching}: Simple string matching for hack-related terms (e.g., ``sys.exit'', ``mock'', ``skip test'')
    \item \textbf{Regex Patterns}: Our 45-pattern detector used in isolation
    \item \textbf{Bag-of-Words + Logistic Regression}: TF-IDF features (max 10,000) with L2-regularized logistic regression
    \item \textbf{DistilBERT (Ours)}: Fine-tuned transformer classifier
\end{itemize}

\subsection{Evaluation Protocol}

\textbf{Metrics:} We report F1 score (primary), precision, recall, and accuracy. For statistical significance, we compute 95\% confidence intervals via bootstrap ($n=1000$) and perform two-tailed t-tests against baselines.

\textbf{Validation Strategy:} We reserve 10\% of the training split (431 samples) as a validation set for threshold tuning and early stopping. Cross-validation is performed on the training portion only; test metrics are reported on the held-out test set which remains untouched during model development.

\textbf{Hyperparameter Selection:} Learning rate, batch size, and number of epochs were selected via grid search on the validation set before any test set evaluation.

%==============================================================================
\section{Results}
%==============================================================================

\subsection{Classification Performance}

\begin{table}[H]
\centering
\caption{Classification performance comparison. Our DistilBERT classifier significantly outperforms all baselines ($p < 0.001$).}
\label{tab:results}
\begin{tabular}{lcccc}
\toprule
Method & F1 & Precision & Recall & Accuracy \\
\midrule
Keyword Matching & 0.1\% & 0.1\% & 5.1\% & 96.2\% \\
Regex Patterns & 4.9\% & 2.6\% & 38.5\% & 96.0\% \\
BoW + LogReg & 7.0\% & 4.0\% & 28.2\% & 96.3\% \\
\midrule
\textbf{DistilBERT (Ours)} & \textbf{89.7\%} & \textbf{89.7\%} & \textbf{89.7\%} & \textbf{99.3\%} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{fig3_benchmark_comparison.png}
    \caption{\textbf{F1 score comparison across detection methods.} Our DistilBERT classifier achieves 89.7\% F1, dramatically outperforming traditional approaches. The gap highlights the importance of semantic understanding for reward hack detection.}
    \label{fig:benchmarks}
\end{figure}

Our DistilBERT classifier achieves 89.7\% F1 on held-out test data, with 95\% confidence interval [84.8\%, 90.0\%]. This represents a \textbf{12.8$\times$ improvement} over the best baseline (BoW + LogReg at 7.0\% F1). The improvement is statistically significant ($p < 0.001$, two-tailed t-test).

\textbf{Cross-Validation.} Five-fold cross-validation on the training set yields 87.4\% $\pm$ 2.9\% F1, confirming model stability across data splits.

\subsection{Per-Category Analysis}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{fig4_category_performance.png}
    \caption{\textbf{Detection performance by hack category.} Test manipulation achieves perfect detection (100\% F1), while mock exploits show 0\% F1 due to underrepresentation in training data (known limitation).}
    \label{fig:categories}
\end{figure}

Performance varies by hack category (Figure~\ref{fig:categories}):
\begin{itemize}
    \item \textbf{Test Manipulation}: 100\% F1 (well-represented, clear patterns)
    \item \textbf{Deceptive CoT}: 90\% F1 (semantic patterns captured)
    \item \textbf{sys.exit() Hacks}: 66.7\% F1 (moderate representation)
    \item \textbf{Mock Exploits}: 0\% F1 (underrepresented in training)
\end{itemize}

The mock exploit failure is a known limitation; these represent only 0.3\% of training samples. Future work will address this through targeted synthetic data generation.

\subsection{Ablation Study}

\begin{table}[H]
\centering
\caption{Detection component ablation. ML-only configuration performs best; pattern detection introduces false positives when combined.}
\label{tab:ablation}
\begin{tabular}{lccc}
\toprule
Configuration & F1 & Precision & Recall \\
\midrule
ML Only & \textbf{89.7\%} & 89.7\% & 89.7\% \\
ML + Pattern (OR) & 23.3\% & 13.8\% & 71.8\% \\
ML + Pattern (AND) & 21.7\% & 72.4\% & 12.8\% \\
Pattern Only & 4.9\% & 2.6\% & 38.5\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Finding}: Pattern detection introduces false positives that degrade performance when combined with ML. We recommend a \textbf{forensic-focused architecture} for production: the ML classifier serves as the primary trigger for intervention (89.7\% F1), while the Pattern Detector acts as an \textit{interpretability layer}, surfacing specific code smells (e.g., \texttt{sys.exit} calls) to aid human reviewers in post-incident analysis.

\subsection{RMGI Parameter Analysis}

\begin{table}[H]
\centering
\caption{RMGI parameter sensitivity on 847 trajectories with labeled transitions. Optimal configuration: window size 10, threshold 0.7.}
\label{tab:rmgi}
\begin{tabular}{ccccc}
\toprule
Window & Threshold & F1 & Recall & FPR \\
\midrule
10 & 0.7 & \textbf{0.452} & 70.0\% & 4.3\% \\
10 & 0.6 & 0.400 & 70.0\% & 5.7\% \\
20 & 0.7 & 0.387 & 54.5\% & 2.9\% \\
5 & 0.7 & 0.364 & 61.8\% & 4.3\% \\
\bottomrule
\end{tabular}
\end{table}

Optimal configuration (window=10, threshold=0.7) achieves 70\% recall for transition detection at 4.3\% false positive rate. This allows early warning while maintaining acceptable precision for operational use.

\textbf{Interpreting RMGI Performance.} Although the absolute F1 score (0.452) appears modest compared to per-step classification, transition detection is a fundamentally harder task: we are identifying rare behavioral shifts across long trajectories rather than classifying individual steps. We therefore interpret RMGI as an \textit{early warning signal} rather than a sole decision mechanism, suitable for triggering human review or additional monitoring.

\subsection{Calibration Analysis}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig6_calibration.png}
    \caption{\textbf{Model calibration analysis.} Left: Reliability diagram showing well-calibrated probabilities (diagonal = perfect calibration). Right: Score distribution by class showing clear separation between clean (blue) and hack (red) samples. Brier score: 0.033, Expected Calibration Error: 0.027.}
    \label{fig:calibration}
\end{figure}

Our classifier produces well-calibrated probabilities (Figure~\ref{fig:calibration}):
\begin{itemize}
    \item \textbf{Brier Score}: 0.033 (lower is better)
    \item \textbf{Expected Calibration Error (ECE)}: 0.027
\end{itemize}

Good calibration is critical for RMGI computation, which relies on continuous scores rather than binary predictions.

\subsection{Threshold Sensitivity}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{fig5_threshold_sensitivity.png}
    \caption{\textbf{Threshold sensitivity analysis.} Precision (blue), recall (orange), and F1 (green) as functions of classification threshold. Optimal threshold is 0.02, reflecting the calibrated low-probability outputs on imbalanced data.}
    \label{fig:threshold}
\end{figure}

Figure~\ref{fig:threshold} shows performance across classification thresholds. The optimal threshold of 0.02 is lower than typical due to our imbalanced dataset and calibrated outputs. This threshold should be recalibrated if deployed on data with different base rates.

%==============================================================================
\section{System Implementation}
%==============================================================================

RewardHackWatch is implemented as a Python package with production-ready components:

\begin{verbatim}
rewardhackwatch/
    core/
        detectors/     # Pattern, AST, ML detection
        judges/        # Claude and Llama LLM judges
        trackers/      # RMGI computation, changepoint detection
    api/               # FastAPI REST server
    dashboard/         # Streamlit interactive UI
    cli.py             # Command-line interface
\end{verbatim}

\textbf{Performance Characteristics:}
\begin{itemize}
    \item \textbf{Inference latency}: $\sim$50ms (CPU), $\sim$10ms (GPU)
    \item \textbf{Memory footprint}: $\sim$500MB (model loaded)
    \item \textbf{Throughput}: $\sim$200 trajectories/second (batched)
\end{itemize}

The system is designed for runtime integration with LLM agent frameworks, enabling continuous monitoring during deployment.

%==============================================================================
\section{Limitations}
%==============================================================================

\textbf{Mock Exploit Detection.}
Our classifier achieves 0\% F1 on mock/monkeypatch hacks due to severe underrepresentation in training data (0.3\% of samples). This is a \textit{data scarcity issue}, not a fundamental architectural flaw. To bridge this gap, future work will employ \textbf{synthetic data generation via red-teaming agents} specifically prompted to bypass tests using \texttt{unittest.mock} and monkeypatching. We hypothesize that fine-tuning on this synthetic corpus will restore sensitivity to these rare but dangerous hack types, as the underlying DistilBERT architecture has demonstrated strong generalization on other hack categories.

\textbf{Threshold Sensitivity.}
The optimal classification threshold (0.02) is calibrated to our dataset's 3.6\% base rate. A static threshold this low may appear fragile and could generate excessive false positives in environments with different hack prevalence. For operational deployment, we recommend \textbf{dynamic thresholding}: periodically recalibrate $\tau$ against the 99th percentile of a verified ``clean'' trajectory window, ensuring the false positive rate remains bounded regardless of distribution shifts. Users should \textit{not} blindly apply the 0.02 threshold without calibration on their specific data.

\textbf{RMGI Temporal Requirements.}
RMGI requires minimum 5 timesteps for reliable correlation estimates. Very short trajectories may produce unstable scores.

\textbf{Causal Limitations.}
RMGI measures temporal correlation between hack and misalignment scores, not causation. While Anthropic \cite{anthropic2025} provides evidence for causal mechanisms, our metric serves as a \textit{proxy} for generalization rather than proof of causality.

\textbf{Adversarial Robustness.}
We have not extensively tested against adversarial evasion. Preliminary experiments with simple paraphrase and case transformations show $<$5\% F1 degradation, but a sophisticated agent aware of our detection patterns could potentially circumvent monitoring.

\textbf{Covert Misalignment.}
As noted by Anthropic \cite{anthropic2025}, 40-80\% of misaligned responses are ``covert''---showing misaligned reasoning but producing safe-looking outputs. Our system detects reasoning patterns but may miss cases where deception is fully hidden from the chain-of-thought.

%==============================================================================
\section{Conclusion}
%==============================================================================

We presented RewardHackWatch, the first open-source runtime detection system targeting reward-hacking generalization in LLM agents. Our key findings:

\begin{enumerate}
    \item \textbf{ML-based detection significantly outperforms traditional methods}: 89.7\% F1 vs.\ 4.9\% for regex patterns, highlighting the importance of semantic understanding.
    \item \textbf{RMGI successfully identifies behavioral transitions}: 70\% recall at 4.3\% FPR enables early warning of hack$\rightarrow$misalignment generalization.
    \item \textbf{Combined detection requires careful design}: Na\"ive combination of ML and pattern detection degrades performance due to false positives.
\end{enumerate}

As LLM agents become more capable and autonomous, monitoring for the emergence of misalignment from seemingly harmless shortcuts becomes critical \cite{amodei2016, hubinger2019}. RewardHackWatch provides a foundation for this monitoring capability.

\textbf{Future Work.}
Promising directions include: (1) probing internal model activations for earlier detection \cite{goldowsky2023}, (2) extending to multi-agent scenarios, (3) developing adversarially robust detection methods, (4) synthetic data generation for rare hack categories, and (5) longitudinal studies tracking generalization dynamics across extended training runs.

\textbf{Open Source.}
Code and data are available at \url{https://github.com/aerosta/rewardhackwatch} to enable the research community to monitor and study reward hacking behaviors in deployed LLM agents.

%==============================================================================
\section*{Acknowledgments}
%==============================================================================

We thank the Anthropic and OpenAI research teams for their foundational work on reward hacking and reasoning monitoring that motivated this research.

%==============================================================================
\begin{thebibliography}{20}
%==============================================================================

\bibitem{amodei2016}
D.~Amodei, C.~Olah, J.~Steinhardt, P.~Christiano, J.~Schulman, and D.~Man\'{e}.
``Concrete problems in AI safety.''
\textit{arXiv:1606.06565}, 2016.

\bibitem{anthropic2025}
M.~MacDiarmid, J.~Uesato, J.~Benton, et al.
``Natural emergent misalignment from reward hacking in production RL.''
\textit{arXiv:2511.18397}, Nov 2025.

\bibitem{betley2025}
J.~Betley, D.~Paleka, et al.
``Emergent misalignment: Narrow finetuning can produce broadly misaligned LLMs.''
\textit{arXiv:2502.17424}, 2025.

\bibitem{goldowsky2023}
N.~Goldowsky-Dill, C.~MacLeod, L.~Sato, and A.~C.~Stickland.
``Localizing lying in Llama: Understanding instructed dishonesty on true-false questions.''
\textit{arXiv:2311.15131}, 2023.

\bibitem{goodhart1984}
C.~A.~E.~Goodhart.
``Problems of monetary management: The UK experience.''
In \textit{Monetary Theory and Practice}, pp. 91--121. Springer, 1984.

\bibitem{greenblatt2024}
R.~Greenblatt, C.~Denison, B.~Wright, et al.
``Alignment faking in large language models.''
\textit{arXiv:2412.14093}, Dec 2024.

\bibitem{hubinger2019}
E.~Hubinger, C.~van Merwijk, V.~Mikulik, J.~Skalse, and S.~Garrabrant.
``Risks from learned optimization in advanced machine learning systems.''
\textit{arXiv:1906.01820}, 2019.

\bibitem{hubinger2024}
E.~Hubinger, C.~Denison, J.~Mu, et al.
``Sleeper agents: Training deceptive LLMs that persist through safety training.''
\textit{arXiv:2401.05566}, Jan 2024.

\bibitem{killick2012}
R.~Killick, P.~Fearnhead, and I.~A.~Eckley.
``Optimal detection of changepoints with a linear computational cost.''
\textit{Journal of the American Statistical Association}, 107(500):1590--1598, 2012.

\bibitem{krakovna2020}
V.~Krakovna, J.~Uesato, V.~Mikulik, M.~Rahtz, T.~Everitt, et al.
``Specification gaming: The flip side of AI ingenuity.''
DeepMind Blog, 2020.

\bibitem{laidlaw2024}
C.~Laidlaw, S.~Singhal, and A.~Dragan.
``Correlated proxies: A new definition and improved mitigation for reward hacking.''
In \textit{International Conference on Learning Representations}, 2024.

\bibitem{langosco2022}
L.~Langosco, J.~Koch, L.~Sharkey, J.~Pfau, and D.~Krueger.
``Goal misgeneralization in deep reinforcement learning.''
In \textit{International Conference on Machine Learning}, 2022.

\bibitem{openai2025}
B.~Baker, J.~Huizinga, D.~Farhi, et al.
``Monitoring reasoning models for misbehavior and the risks of promoting obfuscation.''
\textit{arXiv:2503.11926}, Mar 2025.

\bibitem{pan2022}
A.~Pan, K.~Bhatia, and J.~Steinhardt.
``The effects of reward misspecification: Mapping and mitigating misaligned models.''
In \textit{International Conference on Learning Representations}, 2022.

\bibitem{perez2022}
E.~Perez, S.~Ringer, K.~Lukovsi\={u}t\.{e}, K.~Nguyen, et al.
``Discovering language model behaviors with model-written evaluations.''
\textit{arXiv:2212.09251}, 2022.

\bibitem{shah2022}
R.~Shah, V.~Varma, R.~Kumar, M.~Phuong, et al.
``Goal misgeneralization: Why correct specifications aren't enough for correct goals.''
\textit{arXiv:2210.01790}, 2022.

\bibitem{skalse2022}
J.~Skalse, N.~H.~R.~Howe, D.~Krasheninnikov, and D.~Krueger.
``Defining and characterizing reward hacking.''
In \textit{Advances in Neural Information Processing Systems}, 2022.

\end{thebibliography}

\end{document}
